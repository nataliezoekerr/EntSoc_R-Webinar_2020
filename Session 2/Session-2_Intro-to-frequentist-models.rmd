---
title: 'Session 2: Intro to frequentist models in R'
author: "Natalie Z. Kerr & Brian Lovett"
date: "October 11, 2020"
output:
  html_document:
    toc: yes
    toc_depth: 3
    toc_float:
      collapsed: no
  pdf_document:
    toc: yes
    toc_depth: '3'
fontsize: 12pt
urlcolor: blue
---

<br> 

## Workshop description

Here, we will cover how to run linear models (LMs; e.g., Gaussian/Normal distribution), generalized linear models (GLMs; e.g., binomial, Poisson for count data), and mixed models (LMMs/GLMMs; e.g., fixed vs. random effects). We will also expand on models using Poisson-distributed data by evaluating how to deal with common issues such as when your count data are over-dispersed/under-dispersed (e.g., Poisson vs. Negative Binomial vs. Conway-Maxwell-Poisson), when counts should be represented as rates (e.g., counts per unit time using Poisson offsets), and when your count data are zero-inflated (e.g., 0-inflated regression vs hurdle models). Finally, we will finish the session on appropriate ways to run model selection techniques for finding a winning model from a set of candidate models, using likelihood ratio tests for nested models and Akaike Information Criteria (AIC). This course will not be a statistics course, so people will need to be familiar with most of these models.

<br>

## Packages required

Before running the following code, please open the R Project for the EntSoc R Webinar series in the main folder ("EntSoc_R-Webinar_2020.Rproj"). 

Here, is a list of packages required for this R course. You will need to install these prior to the class, either install from the "packages" panel in R studio or using the function below. 

> install.package("")

Once installed, we can run these packages in advance. I will inform you whenever we are running a function from each of these packages throughout this session. 

```{r Packages required, message=F, warning=F }

library(here) # for navigation among folders
library(tidyverse) # for all tidyverse packages

library(pscl) # for zero-inflated regression models
library(glmmTMB) # for hurdle models
library(lme4) # for mixed models

library(car) # for likelihood ratio tests/marginal hypothesis testing
library(lmtest) # for likelihood ratio tests
library(bbmle) # for AIC

```

<br> 

## Workshop topics

Today, we will cover five main topics in this workshop: 

   1. Linear models 
   2. Model selection approaches
   3. Generalized linear models
   4. Common issues with Poisson-distributed data
   5. Mixed models

We will cover alternative statistical distributions to the Normal and Poisson distributions, when encountering common issues with these data. For background reading that is required for this course, I would highly recommend reading:

* "Ecological models and data in R" by *Ben Bolker*
* "Model Selection and Inference: A Practical Information-Theoretic Approach" by *Kenneth Burnham* and *David Anderson*  
* Some of the first chapters (1-6) of "Program MARK: A Gentle Introduction" by *Gary White* and *Evan Cooch* that provides a good introduction to probability maximum likelihood estimation.

I have listed some other useful resources at the end of this rmarkdown file. Many of my example datasets were used in "Introduction to WinBUgS for Ecologists" by *Marc Kery* that provides both WinBUGS and R code. 

<br>

### 1. Intro to linear models

First, we will cover running linear models (LMs) for normally-distributed (or Gaussian distributed) data. Linear models can be used to carry out single stratum analysis of variance (i.e., intercept-only models), analysis of variance (ANOVA, i.e., differences among groups), regression, and analysis of covariance (ANCOVA).  

For these models, we will use Brown hare (*Lepus europaeus*) data over 17 years (1992-2008) at 56 sites in 8 regions of Switzerland for most of our examples today. Sites vary in area, elevation, and belong to two habitat types (arable and grassland). Mean density is the count1 of hares offset by the area of the site (i.e., mean.density = count1/area). These data are used in the [2010 Marc Kery book](https://www.mbr-pwrc.usgs.gov/software/kerybook/) that contains examples of both R and WinBUGS code.  

```{r Hares data, include = T }

hares <- read.table(here::here("Session 2", "Data", "hares.txt"), header = T)
head(hares)

```

<br>

#### Intercept-only models 

First, we will run a single stratum analysis of variance (aka "intercept-only") model to estimate the mean density of Brown hares across Switzerland.  

```{r Intercept-only model }

dens1 <- lm(mean.density ~ 1, data = hares)
summary(dens1)

```
  
Interpreting the summary output:    
  
* *Call:* model formula.
* *Residuals:* difference between the observed response values and model predicted values. Mean should be zero.  
* *Coefficients:* 
  + Model estimate
  + SE of model estimate
  + t-value (SDs our estimate is from 0)  
  + P-value (i.e., probability of observing a value equal or larger than *t*, i.e., is our model estimate is significantly different from 0?)
* *Residual Standard Error:* average amount that the response will deviate from our model estimate.  

```{r Interpreting the summary output }

mean(dens1$residuals) # mean of the residuals is close to zero

mean(hares$mean.density, na.rm = T) # Mean
sd(hares$mean.density, na.rm = T)/sqrt(nrow(subset(hares, !is.na(mean.density)))) # SE = sd/sqrt(n)

summary(dens1)$sigma / summary(dens1)$coefficient[1] # 80% percentage error 

```

<br>

#### One-way ANOVA

Here, we will run a one-way analysis of variance (ANOVA) model to estimate mean density of brown hares in the two habitat types: arable vs. grassland. 

```{r One-way ANOVA }

dens2 <- lm(mean.density ~ landuse, data = hares)
summary(dens2)

```

The lm() summary also contains three more outputs:   
  
* *Multiple R-squared:* determined how well your model fits your data (aka coefficient of determination), it subtracts the residual error (i.e., variance in the predictor) from the variance in Y (i.e., variance in the response). Here, only 6.3% of the landuse explains the variance in mean density of hares.   
* *Adjusted R-squared:* provides the same information, but adjusts the multiple R-squared value by the number of variables in your model.   
* *F-statistic:* checks that at least one of your coefficients in your model is nonzero.   

For this model, we used what is referred to as "effect parameterization". The dummy variable (or *Intercept*, $\beta_{0}$) is arable land, and the *landusegrass* is difference to grassland relative to the dummy variable (or *slope*, $\beta_{1}$). The mean density in the arable land is `r coef(dens2)[1]` and the mean density in the grassland is `r coef(dens2)[1] + coef(dens2)[2]`, i.e., $y = \beta_{0} + \beta_{1} x$ where *x* is either "1" for grassland or "0" for arable.  

```{r Effects parametrization }

subset(hares, !is.na(mean.density))[70:80,]
model.matrix(dens2)[70:80,]  # each row is an observation used to find MLE 

effects.par <- lm(mean.density ~ 1 + landuse, data = hares)
summary(effects.par)$coefficients # same model as dens2

```

However, we can also use the "means parameterization" approach for our model structure, where each group is *not* in reference to the "dummy" variable.  

```{r Means parameterization }

means.par <- lm(mean.density ~ -1 + landuse, data = hares)
summary(means.par)$coefficients # removes dummy variable

model.matrix(effects.par)[70:80,]  # each row is an observation used to find MLE 
model.matrix(means.par)[70:80, ]

```

<br> 

**Practice example 1:** Using the built-in "ToothGrowth" R dataset, run an ANOVA to estimate tooth growth for three dosages of vitamin C. Run two models using the effects and means parameterization approach.  

```{r Tooth growth dataset }

data("ToothGrowth") 

```

The ToothGrowth dataset contains data from an experiment studying the effect of vitamin C on tooth growth in 60 Guinea pigs. Each animal received one of three dose levels of vitamin C (0.5, 1, and 2 mg/day) by one of two delivery methods: orange juice (OJ) or ascorbic acid (VC).  

```{r Practice 1 - one-way ANOVA }


```

<br>

#### Two-way ANOVA

Here, we will run a two-way ANOVA without an interactive effect of both landuse and region on mean density of Brown hares. 

```{r Two-way ANOVA without interaction }

dens4 <- lm(mean.density ~ region + landuse, data = hares)
coef(dens4)

dens5 <- lm(mean.density ~ -1 + region + landuse, data = hares)
coef(dens5) # mildly more comprehensible 

```

By removing the dummy variable for region ("dens5"), we can see that each region has an estimate for mean density of brown hares in arable land. To obtain a grassland estimate for each region, you will still need to add landusegrass: `rcoef(dens5)[9]` to each region estimate.   

Next, we will run a two-way ANOVA to evaluate the interactive effects of both vitamin C dosage and supplement type on tooth growth in guinea pigs.  

```{r Two-way ANOVA with interaction }

grow1 <- lm(len ~ supp + factor(dose) + supp:factor(dose), data = ToothGrowth)
coef(grow1)

grow2 <- lm(len ~ supp * factor(dose), data = ToothGrowth) # same model as grow1 
coef(grow2)

```

<br>

**Practice example 2:** Using the grasshopper song data, run a two-way ANOVA looking at the effects of origin population (i.e., non-roadside or roadside) and rearing treatment (i.e., noisy vs quiet) on male Bow-winged grasshopper song. These data are from [Lampe et al. 2013](https://besjournals.onlinelibrary.wiley.com/doi/full/10.1111/1365-2435.12215).


```{r Practice 2 - Two way ANOVA }

hoppers <- read.csv(here::here("Session 2", "Data", "GrasshopperSong.csv"))

gh1 <- lm(LocMax ~ Origin*Treat, data = hoppers)

```

<br>

#### Linear regression

Here, we will use the brown hare dataset to run a linear regression evaluating whether mean density of brown hares changes linearly with year in arable land. 

```{r Linear regression }

hares$yearPost <- hares$year - 1991
hares.arable <- subset(hares, landuse == "arable")

dens6 <- lm(mean.density ~ yearPost, data = hares.arable)
summary(dens6)

with(subset(hares, landuse == "arable"), plot(year, mean.density))
points(1992:2008, coef(dens6)[1] + coef(dens6)[2]*1:17, type = "l")

```

Mean density of brown hares does significantly increase over the years. However, we can also run *n*-degree polynomial linear functions. I have choosen to only run a second-degree polynomial relationship (e.g., quadratic function) to evaluate whether there is an intermediate elevation that has the highest mean density of brown hares.  

```{r Polynomial relationship }

dens7 <- lm(mean.density ~ yearPost + I(yearPost*yearPost), data = hares.arable)
summary(dens7)

with(hares.arable, plot(year, mean.density, pch = 19))
points(1992:2008, coef(dens7)[1] + coef(dens7)[2]*1:17 + coef(dens7)[3]*1:17*1:17, type = "l")

```

Note that we need to use the function *I()* for the quadratic term, which treats the variable "as is" rather than an interaction between two variables (as seen in the two-way ANOVA with an interaction). 

<br>

**Practice example 3:** Using the built-in "ToothGrowth" R dataset, run linear regression to determine whether tooth growth changes linearly with vitamin C dosage. 

```{r Practice 3 - Regression }

tg1 <- lm(len ~ dose, data = ToothGrowth)

```

<br>

#### ANCOVA

Here, we are running an analysis of covariance (ANCOVA) to evaluate the interactive effects of landuse and year on mean density of brown hares. 

```{r ANCOVA }

dens8 <- lm(mean.density ~ yearPost*landuse, data = hares) 
summary(dens8)

```

<br>

**Practice example 4:** From the infected individuals only, run an ANCOVA evaluating the interactive effects of sex and kidney fat index (KFI) on the estimated number of *Elaphostrongylus cervi* parasites.

This example uses epidemiological data from Vicente et al. (2006) that took observations on wild boar and red deer reared on a number of estates in Spain. Here, the dataset contains information on the red deer only. Observations on red deer were taken at different farms, months, year (0-5 are 2000-2005, 99 is 1999), and sexes (1 - Male, 2 - Female). For each observation, the length of the animal (LCT, length of head-body), kidney-fat index (KFI), the number of *Elaphostrongylus cervi* parasites (Ecervi), and the presence (0, 1) of Tuberculosis were taken. These data are used in examples from the [Zuur book](https://highstat.com/index.php/a-beginner-s-guide-to-r).  


```{r Practice 4 - ANCOVA }

deer <- read.table(here::here("Session 2", "Data", "Deer.txt"), header = T)

ec1 <- lm(Ecervi ~ KFI*Sex, data = subset(deer, Ecervi > 0))

```


<br>

#### Assumptions of normality

Before continuing with this statistical distribution, we always want to test for the assumptions of normality on our response variable. A linear regression has four assumptions:

  1. *Linearity of the data:* linear relationship between *x* and *y*. (residuals vs fitted values)
  2. *Normality of residuals:* residual errors are normally-distributed. (QQ plot)
  3. *Homogeneity of residuals variance (i.e., homoscedasicity):* constant variance of the residuals. (scale-location plot)
  4. *Independence of residuals error terms:* depends on what may be dependent on your residuals. 

We can test for relevant assumptions using diagnostic plots:

```{r Diagnostic plots }

dens6 <- lm(mean.density ~ year, data = hares.arable)

plot(dens6, 1) # Residuals vs fitted - red line is flat meaning linear relationsip
plot(dens6, 2) # QQ plot - examine whether residuals are normally-distributed
plot(dens6, 3) # Scale-location - homogeneity of variance of residuals (homoscedasicity) if horizontal line with equal spread
plot(dens6, 4) # Cook's distance - identify extreme values and their obs. number
plot(dens6, 5) # Residuals vs leverage - for identifying influential cases or extreme values

hist(hares$mean.density) # left-skewed

```

It is difficult to do a general test for dependence of the residual error term. You will need to know why your residual error term may be dependent: either residuals can correlate with another variable (e.g., check residuals-fitted plot) or residuals can correlate with a nearby residual (e.g., autocorrelation in time series data).

```{r Independence of residual error terms - time series }

LI04.grass <- subset(hares.arable, site == "BE12" & !is.na(mean.density))
acf(LI04.grass$mean.density)

LU01.grass <- subset(hares.arable, site == "GE02" & !is.na(mean.density))
acf(LU01.grass$mean.density)

```

Our residual error terms seem to be independent, particularly from temporal autocorrelation. Regardless, our data do not meet all the assumptions of a linear regression (i.e., normality of residuals since data are negatively (left) skewed and bound by 0 to $\infty$). 

First, we can use the most common approach when our data do not met the assumptions of normality, which is to transform our response variable, *y*. There are many ways to transform your data (e.g., log, square-root, arcsine). However, we will only cover the log transformation of our response variable, *y* (i.e., log(*y*) is normal given *x*):

$$ log(y_{i}) = \beta_{0} + \beta_{1}x_{i} $$

```{r Data transformation }

dens10 <- glm(log(mean.density) ~ year, family = gaussian, data = hares.arable)

plot(dens10, 1) # linear relationship between x and y (good)
plot(dens10, 2) # residuals are not normality-distributed, mild right-skew (ok)
plot(dens10, 3) # mild constant variance of the residuals (ok)

hist(hares$mean.density) # negatively skewed/left skewed
hist(log(hares$mean.density)) # positively skewed/right skewed

```

Instead of transforming our response to fit a statistical distribution (i.e., log(*y*) is normal given *x*), we can choose a statistical distribution that fits our data. First, we might want to try the log-link Gaussian distribution (i.e., mean of log(*y*) responses linearly to *x*), such that:

$$ln(\mu) = \beta_{0} + \beta_{1}x $$

```{r Log-link Gaussian }

dens11 <- glm(mean.density ~ year, family = gaussian(link = "log"), data = hares)

plot(dens11, 1) # relationship between x and y seems linearly (good)
plot(dens11, 2) # residuals are not normality-distributed, left-skewed (bad)
plot(dens11, 3) # constant variance of the residuals (ok)

```

Compared to log-transformation of the *y* values, the log-link Gaussian does not seem to perform better when checking the assumptions of a linear regression. 

We could also use the Gamma distribution that allows for non-negative, skewed, continuous data that are bound by 0 to infinity. The Gamma distribution (either log Gamma or inverse Gamma) assumes heavier tailed/skewed distribution, particularly the inverse Gamma distribution.  


```{r Log-link Gamma distribution }

dens12 <- glm(mean.density ~ year, family = Gamma(link = "log"), data = hares)
plot(dens12, 1) # relationship between x and y seems linearly (good)
plot(dens12, 2) # residuals are not normality-distributed, left-skewed (bad)
plot(dens12, 3) # hetereoscedasicity should exist, since the variance changes with the mean (bad)

```

The Gamma distribution (or any distribution with a scale term) does not need to worry about the same assumptions of a linear regression, except the deviance residuals should be normal and hetereoscadicity should be observed.

<br>

**Practice example 5:** Test the assumptions of normality for the model in practice 3 (i.e., model without an interactive term). Then, find the appropriate approach to deal with any assumptions that are not met.  


```{r Practice 5 - Testing assumptions of linear regression }

ec2 <- lm(Ecervi ~ KFI + Sex, data = subset(deer, Ecervi > 0))
plot(ec2)

ec3 <- lm(log(Ecervi) ~ KFI  + Sex, data = subset(deer, Ecervi > 0))
plot(ec3)

ec4 <- glm(Ecervi ~ KFI + Sex, family = Gamma, data = subset(deer, Ecervi > 0))
plot(ec4)

```


<br>

### 2. Model selection 

Here, I will outline the general procedure for model selection in two parts: selecting the appropriate probability distribution and selecting variables in your model. 

<br>

#### Select the probability distribution

  a. Based on first principles  

![Source: B. Bolker (2008) Ecological Models and Data in R, Princeton University Press, Princeton, New Jersey, USA](SelectingProbabilityDistributions_FirstPrinciples.jpg)

<br>

  b. Based on analysis of data  
   
We could also select the appropriate probability distribution from analyzing our data. To recap, we ran four models to evaluate whether mean density of brown hares changes linearly with year. We can use Akaike Information Criteria to rank our candidate models:

$$ AIC_{model} = 2k - 2LL $$
where *k* is the number of parameters in the model and *LL* is the log likelihood of the model. AIC decreases with increasing model fit (*LL*) but penalizes for the number of parameters. The lower the AIC, the better the model. 

```{r Analysis of data }

dens7 <- lm(mean.density ~ year, data = hares.arable)
dens10 <- glm(log(mean.density) ~ year, family = gaussian, data = hares.arable)
dens11 <- glm(mean.density ~ year, family = gaussian(link = "log"), data = hares)
dens12 <- glm(mean.density ~ year, family = Gamma(link = "log"), data = hares)

ICtab(dens7, dens10, dens11, dens12, base = T)

```

The common rule would be to choose the most parsimonious model within 2 dAIC of the winning model. 

**Note:** If multiple models fall witin 2 dAIC of the winning model, you can choose to do something referred to as "model averaging". Model averaging is not ideal for model with different probability distributions. It would be better for averaging multiple winning models, such a linear and quadratic function since the model falls somewhere between those functions. The *bbmle::ICtab()* function has an argument for computing IC weights that allows for model averaging. For more information on model averaging, see Anderson and Burham 2002. 

<br>

#### Selecting variables

For selecting variables in your model, you can do model selection in two ways: Akaike Information Criteria (AIC) for competing models or likelihood ratio test (LRT) of nested models. 
  
  a. AIC analyses of competing models
  
Here, we might evaluate whether infection intensity of *Elaphostrongylus cervi* parasites in red deer correlated better with length of the animal (LCT) or kidney fat index (KFI). 

```{r AIC of competing models }

ec5 <- lm(log(Ecervi) ~ KFI, data = subset(deer, Ecervi > 0))
ec6 <- lm(log(Ecervi) ~ LCT, data = subset(deer, Ecervi > 0))

ICtab(ec5, ec6)

```

<br>

  b. LRT and "marginal hypothesis testing" of a saturated model. 

For nested models, we may want to use marginal null hypothesis testing of a saturated model to determine significant effects. For example, we might want to evaluate whether Bay Checkerspot caterpillars reared on two host plant (i.e., Indian paintbrush and English Plantain) that were either sprayed or not sprayed with herbicide treatment affected their mass.  

```{r LRT }

herb <- read.csv(here::here("Session 2", "Data", "HerbicideCaterpillars.csv"))

sat_mod <- glm(Mass ~ Host + Treat + Host:Treat, family=gaussian, data = herb) 
Anova(sat_mod)

add_mod <- glm(Mass ~ Host + Treat, family=gaussian, data = herb) 
host_mod <- glm(Mass ~ Host, family=gaussian, data = herb)
treat_mod <- glm(Mass ~ Treat, family=gaussian, data = herb) 

lrtest(sat_mod, add_mod)
lrtest(add_mod, host_mod)
lrtest(add_mod, treat_mod)

```

<br>

### 3. Intro to generalized linear models

Generalized linear models (GLMs) connect a mean of the response to its predictors in a linear way through *link functions*. Therefore, it produces coefficients of a linear relationship on the link function scale. Instead of transforming data to fit a normal distribution, such as the log transformation

$$\mu_{ln(y)} = \beta_{0} + \beta_{1}x_{1} + ...$$
$$ln(y_{i}) = \beta_{0} + \beta_{1}x_{i} + ...$$

GLMs allows data to follow alternative error distributions, such as a Poisson error distribution on a natural log link function scale:   

$$ln(\mu_{y}) = \beta_{0} + \beta_{1}x_{1} + ...$$

<br>

#### Binomial distribution

Here, we will use generalized linear models to estimate a response that is binomially-distributed. A binomial response can either be at the individual-level (i.e., presence/absence) or at the group-level (e.g., k events and N trials).  

For our first example, we will be using data from a laboratory diapause experiment to estimate the probability of entering diapause as a function of photoperiod. I put eggs from the mustard white butterfly (*Pieris oleracea*) in five photoperiod treatments, then measured at the individual-level whether they survived (1 for survival, 2 for died) and whether they diapaused (e.g., 1 for entering diapause, 0 for eclosing) conditioned on surviving. These data were used in [Kerr et al. 2020](https://onlinelibrary.wiley.com/doi/abs/10.1111/gcb.14959).  

We can run a logit-link GLM to estimate the probability of diapausing conditioned on survival as a function of photoperiod, as follows: 

```{r Individual-level binary response - Presence/absence }

diap <- read.csv(here::here("Session 2", "Data", "MW_DiapausingExp.csv"))
head(diap)

pd1 <- glm(Diapause ~ Treatment, family = binomial, data = subset(diap, Sex == "female"))
summary(pd1)
Anova(pd1)

light.vals <- seq(min(diap$Treatment), max(diap$Treatment), length.out = 50)
pd.logit <- coef(pd1)[1] + coef(pd1)[2]*light.vals
pd.pred <- exp(coef(pd1)[1] + coef(pd1)[2]*light.vals)/(1 + exp(coef(pd1)[1] + coef(pd1)[2]*light.vals)) # manual back-transformation

plot(light.vals, pd.logit, type = "l", ylab = "Diapause probability on the logit-link scale", xlab = "Photoperiod")
plot(light.vals, plogis(pd.logit), type = "l", ylab = "Diapause probability", xlab = "Photoperiod")

```

First, the summary output includes two more outputs for a GLM: 
* *Null deviance:* is the 2(LL(Saturated Model) - LL(Null Model)), where the saturated model assumes each data point has its own parameter (*n* parameters) and the null assumes one parameter for all data (1 parameter).
* *Residual deviance:* is the 2(LL(Saturated Model - LL(Proposed Model)) and refers the goodness-of-fit of the proposed model, where your data can be explained by *p* parameters and an intercept term. 

To compare your null with the proposed, you can calculate the chi-squared value $\chi^2 = null deviance - residual deviance$ and the degrees of freedom $df Proposed - df Null$.


For our second example, we will be using data from an overwintering experiment to estimate the overwinter survival of diapausing pupa of the mustard white butterfly. Each bug dorm had a fall count of diapausing pupa (*N*, trials) and a spring count of the number of emerging butterflies (*k*, events).

We can run an intercept-only logit-link GLM to estimate the overwinter survival of the mustard white butterfly, as follows: 

```{r Group-level binary response - k/N }

surv <- read.csv(here::here("Session 2", "Data", "MW_OverwinterSurv.csv"))
head(surv)

ws1 <- glm(cbind(Spring.count, Fall.count - Spring.count) ~ 1, family = binomial, data = surv)
summary(ws1)  

plogis(coef(ws1)) # 13% probability of surviving the winter

```

I think the group-level binary response has many potential applications:

For example, I have used it to estimate the trap color of Venus flytraps based on crude categorization of color along a spectrum. 

![Source: B. Trap color of Venus flytraps from four populations (or eight sub-sites) across their range](Data\Phenotypic-site-differences.jpg)

The colors of traps were one of seven categories ranging from green, green-pink, pink, pink-red, red, red-dark red, and dark red. The total number of trials was 6, where green was 0 successes + 6 failures and dark red was 6 successes + 0 failtures. 

> cbind(0, 6) # green
> cbind(3, 3) # pink-red
> cbind(6, 0) # dark red

![Source: B. Trap color of Venus flytraps from four populations (or eight sub-sites) across their range](Data\VFT_TrapColor_Sites.jpg) 

Another example could be estimating the day of spring emergence for a diapausing insect, where *N* trials is the 365 days of a year (assuming not a leap year) and *k* events is the number of days until emergence. In other words, the number of successes would be days from January 1st that it took to emerge and the number of failures would be the remaining days of the year. Then, you would get some probability (into the year) that an insect would emerge from diapause. You could multiple that probability estimate by the number of days in a year to convert back into "days until spring emergence". 

This binary "group-level" format has many potential ecological applications. 
 
<br>

**Practice example 6:** Using the deer data, run a logit-link GLM to estimate the probability that a red deer will have a parasite given its kidney fat index (KFI) and sex. 

```{r Practice 6 - Probability of infection }

deer$P.Ecervi <- deer$Ecervi
deer$P.Ecervi[deer$P.Ecervi > 0] <- 1

pec1 <- glm(P.Ecervi ~ KFI*Sex, family = binomial, data = deer)

```

<br>

#### Poisson distribution

Here, we will be exploring running log-link GLMs using the Poisson error distribution. Here, we have counts of wolves in each year from 1982 to 2012 across five US states.   

```{r Wolves data }

wolves <- read.csv(here::here("Session 2", "Data", "NRMwolves.csv"))

```

First, we will evaluate whether the total number of wolves has changed over a 10-year period from 1994-2003. 

```{r Number of Wolves - 10 years }

with(wolves[13:22,], plot(year, num.wolves))

wv10a <- glm(num.wolves ~ year, family = poisson(link = "identity"), data = wolves[13:22,])
coef(wv10a) # year is the change in the # of wolves per year 

wv10b <- glm(num.wolves ~ year, family = poisson(link = "log"), data = wolves[13:22,])
exp(coef(wv10b)) # slope is the population growth rate

with(wolves[13:22,], plot(year, num.wolves))
points(1994:2003, coef(wv10a)[1] + coef(wv10a)[2]*1994:2003, type = "l", col = "blue") # linear growth
points(1994:2003, exp(coef(wv10b)[1] + coef(wv10b)[2]*1994:2003), type = "l", col = "red") # unlimited expontential growth 

```

Second, we will evaluate whether the total number of wolves has changed over the full 30-year period from 1983 to 2012. 

```{r Number of Wolves - 30 years }

with(wolves, plot(year, num.wolves))

wv30a <- glm(num.wolves ~ year, family = poisson(link = "log"), data = wolves[2:31,]) # unlimited exp growth
coef(wv30a) 

wv30b <- glm(num.wolves ~ year + I(year*year), family = poisson(link = "log"), data = wolves[2:31,]) # quadratic function model

with(wolves[2:31,], plot(year, num.wolves))
points(1983:2012, exp(coef(wv30a)[1] + coef(wv30a)[2]*1983:2012), type = "l", col = "blue") 
points(1983:2012, exp(coef(wv30b)[1] + coef(wv30b)[2]*1983:2012 + coef(wv30b)[3]*1983:2012*1983:2012), type = "l", col = "red") 

```

<br>

**Practice example 7:** Using the "NewEggs_2015" R object file, run a Poisson GLM to estimate the number of new eggs laid (in bumblebee colonies, *Bombus vosnesenskii*) as a function of days of the experiment ("Days"). Evaluate whether a model with a linear or quadratic function better fits these data. Then, plot the raw data with model predicted values from both models.  

> readRDS() # for loading R object files

```{r Practice 7 - Poisson distribution }

eggs <- readRDS(here::here("Session 2", "Data", "Bumblebee_NewEggs.rds"))

bb_eggs1 <- glm(new.eggs ~ Days + I(Days*Days), family = poisson, data = eggs)
bb_eggs2 <- glm(new.eggs ~ Days, family = poisson, data = eggs)

Anova(bb_eggs1)
lrtest(bb_eggs1, bb_eggs2)

with(eggs, plot(Days, new.eggs, pch = 19))
points(30:120, exp(coef(bb_eggs1)[1] + coef(bb_eggs1)[2]*30:120 + coef(bb_eggs1)[3]*30:120*30:120), type = "l")
points(30:120, exp(coef(bb_eggs2)[1] + coef(bb_eggs2)[2]*30:120), type = "l", col = "red")


```


<br>

### 4. Common issues with count data

Here, we will cover how to deal with three common issues that you may encounter with Poisson-distributed data: (1) when counts should be represented as rates, (2) when count data are over- or under-dispersed for the Poisson distribution variance, and (3) when count data are zero inflated. 

<br>

#### ... when counts should be rates 

A common issue in ecology is when each observtion of count data are not always equally in represented. For example, you may find yourself in a situation where each observation is collected over different lengths of time or you count a number out of a total number.  

Here, we will use the [Audubon Christmas bird count](www.christmasbirdcount.org) 2013 data on Northern Flickers across New England. Since observation periods differ, we want to model our counts are rates (# per hour of observation).

```{r Poisson offsets - flickers }

flickers <- read.csv(here::here("Session 2", "Data", "NE_flickers.csv"))
head(flickers)

nf1 <- glm(Count ~ 1, offset = log(hours), family = poisson, data = flickers)
summary(nf1)

exp(coef(nf1)) # 0.07 birds per hour

```

We can also use offsets for running a process error model for evaluating population dynamics, where the previous time step offsets the counts in the next step. Therefore, you get an estimated growth rate for each time step of the series. 

```{r Poisson offsets - wolves for process error model }

wv30c <- glm(num.wolves[2:31] ~ 1, offset = log(num.wolves[1:30]), family = poisson(link = "log"), data = wolves) 
             
with(wolves[2:31,], plot(year, num.wolves, ylim = c(0, 2000)))
points(1983:2012, exp(coef(wv30a)[1] + coef(wv30a)[2]*1983:2012), type = "l", col = "blue") # observation error
points(1983:2012, exp(coef(wv30b)[1] + coef(wv30b)[2]*1983:2012 + coef(wv30b)[3]*1983:2012*1983:2012), type = "l", col = "red")
points(1983:2012, predict(wv30c, type = "response"), type = "l", col = "green") # process error

```

<br> 

**Practice example 8:** Similarly, we could also model the brown hare counts per area as a Poisson offset instead of log(mean.density) using the normal distribution. Do you find similar estimates for mean density for the two landuse types when using Poisson distribution compared to log-transformed data?

```{r Example 8 - Poisson offsets }

dens12 <- glm(count1 ~ 1, offset = log(area), family = poisson, data = hares)
summary(dens12)

dens13 <- glm(log(mean.density) ~ 1, family = gaussian, data = hares)

exp(coef(dens12)) # 4.4 brown hares per hectare 
exp(coef(dens13)) # 3.5 brown hares per hectare

ICtab(dens12, dens13)

```

<br>

#### ... when data are over-dispersed 

A common issue when fitting Poisson models is overdispersion, i.e., when count data has more variability than expected for a Poisson distribution. For Poisson models, data are less likely to be underdispersed for this given distribution. However, this is still likely to happen for biological data.  

To test for overdispersion, the residual deviance should be equal to the degrees of freedom. In other words, the ratio of residual deviance/degrees of freedom should be equal to 1. If the ratio is greater than 1, then your data is more dispersed than the Poisson error distribution permits. If the ratio is less than 1, then your data are less dispersed than the Poisson error distribution.  

Let's test whether flicker counts are overdispersed, using the "intercept-only" model from the Poisson offset example #1. 

```{r Testing for overdispersion - flickers }

nf1 <- glm(Count ~ 1, offset = log(hours), family = poisson, data = flickers) # model above
summary(nf1)

nf1$deviance/nf1$df.residual # overdispersed

```

Let's test whether hare counts using Poisson offset model was overdispersed. 

```{r Testing for overdispersion - brown hares }

dens12 <- glm(count1 ~ 1, offset = log(area), family = poisson, data = hares) # model above
summary(dens13)

dens12$deviance/dens12$df.residual

```


In both cases, these data are overdispersed for the Poisson distribution. 

There are multiple ways to deal with overdispersed count data, such as [observation-level random effects](https://peerj.com/articles/616/), Conway-Maxwell-Poisson distribution, or the negative binomial distribution. Here, we will use the negative binomial distribution, which is the most common approach for overdispersion. A negative binomial distribution is the average of different Poisson distributions with different means. Unlike the Poisson where the mean is equal to the variance, the variance for the negative binomial ....

Let's re-run the intercept-only Poisson offset model for flicker counts using a negative binomial distribution. 

```{r Negative binomial distribution - flickers }

nf2 <- glmmTMB(Count ~ 1, offset = log(hours), family = nbinom2, data = flickers)
summary(nf2)$coefficients$cond # allows for greater variance
summary(nf1)$coefficients

```

Let's also re-run the intercept-only Poisson offset model for brown hare counts using:

* Negative binomial distribution
* Conway-Maxwell-Poisson distribution

compare this model with the Poisson offset and log-transformed mean density of brown hares. 

```{r Negative binomial distribution - brown hares }

dens14 <- glmmTMB(count1 ~ 1, offset = log(area), family = nbinom1, data = hares)
dens15 <- glmmTMB(count1 ~ 1, offset = log(area), family = compois, data = hares)
ICtab(dens12, dens13, dens14, dens15)

```

<br>

#### ... when data are under-dispersed 

Undispersion is far less common than overdispersion, but it can be often encountered in ecology with species that have small clutch/litter sizes. For example, when a bird may only lay up to 6 eggs per clutch.

Here, we have a dataset on begging behaviour of nestling barn owls that may have an example of underdispersed count data. Roulin and Bersier (2007) looked at nestlings’ response to the presence of the mother and the father. Using microphones inside and a video camera outside the nests, studying vocal begging behaviour when the parents bring prey for 27 nests. We use ‘sibling negotiation,’ defined as the number of calls by the nestlings in the 30-second interval immediately prior to the arrival of a parent, divided by the number of nestlings. Data were collected between 21.30 hours and 05.30 hours on two consecutive nights. The variable ArrivalTime indicates the time at which a parent arrived at the perch with prey.  

Here, we just want to evaluate whether food treatment impacts the brood size of barn owls. 

```{r Testing for underdispersion - brood size of owls }

owls <- read.table(here::here("Session 2", "Data", "owls.txt"), header = T)
owls.sum <- owls %>% group_by(Nest, FoodTreatment) %>% distinct(BroodSize) 
 
bs1 <- glm(BroodSize ~ FoodTreatment, family = poisson, data = owls.sum)
summary(bs1)
Anova(bs1)

summary(bs1)$deviance/summary(bs1)$df.residual # underdispersed, < 1
  
```

To address underdispersed count data, the most appropriate distribution would be the Conway-Maxwell-Poisson distribution that adds a parameter to the Poisson distribution to account for either underdispersion or overdispersion.  

```{r Conway-Maxwell-Poisson (CMP) distribution }

bs2 <- glmmTMB(BroodSize ~ FoodTreatment, family = compois, data = owls.sum)
ICtab(bs1, bs2)

```

<br>

**Practice example 9:** From practice example 7, re-run the Poisson GLM for new eggs laid as a quadratic function of days of experiment ("Days") including offset of days between brood mapping ("Days.off") and evaluate whether these data are either underdispersed or overdispersed for a Poisson distribution. 

```{r Practice 9 - Overdispersion/underdispersion }


```

<br>

#### ... when counts are zero-inflated

Here, we will run two different models when you encounter many zeros in your count data (i.e., zero-inflated count data). We can take two potential approaches: zero-inflated regression or hurdle model. The first assumes that not all zeros are "true" zeros and that some are part of the Poisson process. This can commonly occur due to observation error. For example, your count data may be number of butterflies seen, but you are not sure that not seeing an individual means that there were actually no individuals present. For the zero-inflated models, the binomial process may determine whether a location is actually suitable habitat and the count process may represent the quality of the suitable habitat. However, remember that a count of 0 may not necessarily mean that it is a not suitable habitat. 

A zero-inflated Poisson model is required for this process, since not all zeros are true. Therefore, a zeroinflated model evaluates the nonzero and zero process together by evaluating the probability that a zero comes from the main "nonzero" distribution vs. the binomial distribution (i.e., an excess zero). 

> pscl::zerinfl(y ~ x_count | x_zero)

```{r ZIP using pscl }

hist(flickers$Count) # excess zeros

ZiP1 <- zeroinfl(Count ~ 1 | 1, offset = log(hours), dist = "negbin", data = flickers)
summary(ZiP1)

ZiP2 <- zeroinfl(Count ~ Latitude | 1, offset = log(hours), dist = "negbin", data = flickers)
summary(ZiP2)
Anova(ZiP2)

ZiP3 <- zeroinfl(Count ~ Latitude | Latitude, offset = log(hours), dist = "negbin", data = flickers)

```

Note that the 'glmmTMB' package allows for both zeroinflated mixed models and hurdle mixed models.

```{r ZIP using glmmTMB }

ZiP4 <- glmmTMB(Count ~ Latitude, offset = log(hours), ziformula = ~1, family = "nbinom2", data = flickers)
summary(ZiP4)$coefficients

ZiP5 <- glmmTMB(Count ~ Latitude, offset = log(hours), ziformula = ~., family = "nbinom2", data = flickers)
summary(ZiP5)$coefficients

```


For hurdle models, the nonzero and zero processes are modelled separately, and therefore, we assume all zeros are true zeros. For the deer data, let's assume that method for detecting *Elaphostrongylus cervi* parasites in red deer is very accurate and all zeros are true zeros. Here, let's evaluate whether the probability of infection dependent on sex and the infection intensity is dependent on kidney fat index (KFI).  


```{r Hurdle models using glm }

with(deer, hist(Ecervi)) # zero-inflated
with(subset(deer, P.Ecervi == 1), hist(Ecervi))
with(subset(deer, P.Ecervi == 1), hist(log(Ecervi)))

h_zero <- glm(P.Ecervi ~ Sex, family = binomial, data = deer)
Anova(h_zero)
coef(h_zero)

h_nzero <- glm(log(Ecervi) ~ KFI, family = gaussian, data = subset(deer, P.Ecervi == 1))
Anova(h_nzero)
summary(h_nzero)

non_hurdle <- glm(log(Ecervi + 0.01) ~ Sex*KFI, family = gaussian, data = deer)
Anova(non_hurdle)

# Is the hurdle model better?
AIC(non_hurdle)
2*(length(coef(h_zero)) + length(coef(h_nzero))) - 2*(logLik(h_zero) + logLik(h_nzero))

```

The glmmTMB package also allows for hurdle models for count data using the "truncated_poisson". Here, we are using the owl dataset. 

```{r Hurdle models using glmmTMB }

Owls <- read.table(here::here("Session 2", "Data", "owls.txt"), header = T)
table(Owls$SiblingNegotiation)

h_mod_pois <- glmmTMB(SiblingNegotiation ~ BroodSize*FoodTreatment, ziformula = ~FoodTreatment, 
                      family = truncated_poisson(link = "log"), data = Owls)
h_mod_nbin1 <- glmmTMB(SiblingNegotiation ~ BroodSize*FoodTreatment, ziformula = ~FoodTreatment, 
                      family = truncated_nbinom1(link = "log"), data = Owls)
h_mod_nbin2 <- glmmTMB(SiblingNegotiation ~ BroodSize*FoodTreatment, ziformula = ~FoodTreatment, 
                      family = truncated_nbinom2(link = "log"), data = Owls)
ICtab(h_mod_pois, h_mod_nbin, h_mod_nbin1) # do we need to account for overdispersion?

mod_nbin <- glmmTMB(SiblingNegotiation ~ BroodSize*FoodTreatment, family = nbinom1, data = Owls)
ICtab(mod_nbin, h_mod_nbin) # are our data zero-inflated?

h_mod1 <- glmmTMB(SiblingNegotiation ~ BroodSize*FoodTreatment, ziformula = ~FoodTreatment, 
                      family = truncated_nbinom1(link = "log"), data = Owls) # winning model
Anova(h_mod1) 

h_mod2 <- glmmTMB(SiblingNegotiation ~  BroodSize + FoodTreatment, ziformula = ~FoodTreatment, 
                  family = truncated_nbinom1(link = "log"), data = Owls)
h_mod3 <- glmmTMB(SiblingNegotiation ~  BroodSize + FoodTreatment, ziformula = ~1, 
                  family = truncated_nbinom1(link = "log"), data = Owls)
lrtest(h_mod2, h_mod3)

```

<br>

**Practice example 10**: Similar to the Northern Flickers, the Eastern bluebird data was also collected across New England from the [Audubon Christmas bird count](www.christmasbirdcount.org). Eastern Bluebirds have breeding grounds in the northern half of the United States, but all year grounds south of Massachusetts down to Texas and exclusive wintering grounds in Texas and northern Mexico.

Using the "bluebird.csv" data, choose whether a zeroinflated or hurdle model of bird counts with an offset of observation hours is more appropriate for these data. Evaluate both latitude on the zero (i.e., probability that it has migrated) and nonzero (i.e., # of all-year resident birds) process parts. 

```{r Practice 10 - excess zeros }




```


<br>

### 5. Mixed models

Here, we evaluated models with only fixed effects: parameters with fixed or non-random effects (i.e., fixed group means). Fixed effects are generally the covariates of interest. Here, we will run models with covariates that evaluate both *fixed* and *random* effects called "**mixed models**" or "**mixed effects models**". These are a form of hierarchical models where the fixed effects are no longer assumed to be independent, but they come from a second distribution with some mean and variance of some "hyperparameter/s". For example, a simple fixed-effects ANOVA with estimating group means: 

$$ y_{i} = \alpha_{j(i)} + \epsilon_{i} $$
$$ \epsilon_{i} ~ Normal (0, \sigma^2) $$
where $\alpha_{j(i)$ is the mean of response variable $y_{i}$ for population *j* and $\epsilon_{i}$ is the random deviance of *i* from its population mean $\alpha_{j(i)$. 

For a random-effects ANOVA, the $\alpha_{j(i)$ paramaters come from a second distribution with some mean $\mu$ and variance $\tao^2$:   

$$ \alpha_{j(i) ~ Normal(\mu, \tao^2) $$

**NOTE:** a random-effects model rarely contains less than than five populations/groups, since estimating variance with sample size less than five will not result in very precise estimates. 

A common way to distinguish fixed from random effects is respectively based on what is the subject of interest vs some random "latent" variable that may result in greater variance but is *not* of interest. 

<br>

#### Random-coefficient models

Kery 2010 (CH 12) outlines four potential random-coefficient models: 

1. Only intercepts are random, but all slopes are identical for all "random" groups. 
2. Only slopes are random, but all intercepts are identical for all "random" groups. (*this model is not sensible model in most circumstances*)
3. Both intercepts and slopes are random, but they are independent. 
4. Both intercepts and slopes are random, and there is a correlation between them. 

Here, we will run all four combinations of random-coefficient models for the brown hares dataset using the log mean density of brown hares as a function of rescaled year ("yearPost") using `lme4::glmer()`.

```{r Random-intercept model }

dens_re1 <- lmer(log(mean.density) ~ yearPost + (1 | region), data = hares)
summary(dens_re1)

ranef(dens_re1)

# Plotting each intercept
year.vals <- 1:17
re_list1 <- list()
for(i in 1:length(unique(hares$region))){
  re_list1[[i]] <- data.frame(region = rep(unique(hares$region)[i], 17), yearPost = year.vals, 
                         int = rep(ranef(dens_re1)$region[i,1], 17), slope = rep(fixef(dens_re1)[2], 17))
}
re_dat1 <- do.call("rbind", re_list1)
re_dat1$pred <- re_dat1$int + re_dat1$slope*re_dat1$yearPost

ggplot(re_dat1, aes(x = yearPost, y = pred, col = region)) + geom_line() + 
  geom_point(hares, mapping = aes(x = yearPost, y = log(mean.density), col = region))

```

```{r Random-slope model }

dens_re2 <- lmer(log(mean.density) ~ yearPost + (0 + yearPost | region), data = hares)
summary(dens_re2)

ranef(dens_re2)

# Plotting each intercept
re_list2 <- list()
for(i in 1:length(unique(hares$region))){
  re_list2[[i]] <- data.frame(region = rep(unique(hares$region)[i], 17), yearPost = year.vals, 
                         int = rep(fixef(dens_re2)[1], 17), slope = rep(ranef(dens_re2)$region[i,1], 17))
}

re_dat2 <- do.call("rbind", re_list2)
re_dat2$pred <- re_dat2$int + re_dat2$slope*re_dat2$yearPost

ggplot(re_dat2, aes(x = yearPost, y = pred, col = region)) + geom_line() + 
  geom_point(hares, mapping = aes(x = yearPost, y = log(mean.density), col = region))

```

```{r Random-intercept and -slope, but no correlation }

dens_re3 <- lmer(log(mean.density) ~ yearPost + (1 | region) + (0 + yearPost | region), data = hares)
summary(dens_re3)

ranef(dens_re3)

# Plotting each intercept
re_list3 <- list()
for(i in 1:length(unique(hares$region))){
  re_list3[[i]] <- data.frame(region = rep(unique(hares$region)[i], 17), yearPost = year.vals, 
                         int = rep(ranef(dens_re3)$region[i,1], 17), 
                         slope = rep(ranef(dens_re3)$region[i,2], 17))
}
re_dat3 <- do.call("rbind", re_list3)
re_dat3$pred <- re_dat3$int + re_dat3$slope*re_dat3$yearPost

ggplot(re_dat3, aes(x = yearPost, y = pred, col = region)) + geom_line() + 
  geom_point(hares, mapping = aes(x = yearPost, y = log(mean.density), col = region))

```

```{r Random-intercept and -slope with a correlation }

dens_re4 <- lmer(log(mean.density) ~ yearPost + (yearPost | region), data = hares)
summary(dens_re4)

ranef(dens_re4)


# Plotting each intercept and slope
re_list4 <- list()
for(i in 1:length(unique(hares$region))){
  re_list4[[i]] <- data.frame(region = rep(unique(hares$region)[i], 17), yearPost = year.vals, 
                         int = rep(ranef(dens_re4)$region[i,1], 17),  
                         slope = rep(ranef(dens_re4)$region[i,2], 17))
}
re_dat4 <- do.call("rbind", re_list4)
re_dat4$pred <- re_dat4$int + re_dat4$slope*re_dat4$yearPost

ggplot(re_dat4, aes(x = yearPost, y = pred, col = region)) + geom_line() + 
  geom_point(hares, mapping = aes(x = yearPost, y = log(mean.density), col = region))


ICtab(dens_re1, dens_re2, dens_re3, dens_re4)

```

**Practice example 11:** 

```{r Practice 11 - Random-coefficient models }


```

#### Nested random effects

Here, we know that each of the 56 sites are nested within the eight regions of Switzerland that were censused for brown hares from 1992 to 2008. Here, we will run a nested random-intercept model.

```{r Nested random effects }

dens_re5 <- lmer(log(mean.density) ~ yearPost + (1 | region/site), data = hares)
summary(dens_re5)

ranef(dens_re5)

```


**Practice example 12:** 

```{r Practice 12 - Nested random effects }


```


### 6 Introduction to simulating data

Here, I will introduce you to the concepts of density (e.g., `dnorm()`), distribution function (e.g., `pnorm()`), quantile function (e.g., `qnorm()`) and random generation (e.g., `rnorm()`) functions for different distributions. 
```{r Normal distribution }

n <- 100000 # sample size
mu <- 600 # mean body mass of male pelegrines 
sd <- 30 # SD of body size

sample <- rnorm(n = n, mean = mu, sd = sd)
head(sample) # vector of randomly generated numbers

hist(sample, col = "grey")

dnorm(x = 650, mean = mu, sd = sd) # what is the probability of a male pelegrine being 650 g?
pnorm(q = 650, mean = mu, sd = sd) # cumulative density function, 95% of male pelegrines will weigh 650 or less
qnorm(p = 0.90, mean = mu, sd = sd) # what is the 90% percentile of male pelegrine mass?

```


```{r Binomial distribution }


```


```{r Poisson distribution }


```



## Other frequentist models

We did not delve into the world of multivariate statistics for understand which set of response variables explains the most variation in your data, such as Principal Component Analysis (PCA), Linear discriminant Analysis (LCA), or Partial Least Square Regression (PLSR). 

Two good resources for multivariate statistics would be *A Primer of Ecological Statistics* by Gotelli and Ellison (Ch 12) and the first chapter of *A Beginner's Guide to Generalized Additive Models with R* by Zuur, which is also a good introduction to GAMs where your linear predictor depends on some smooth function (rather than parametric function) of some covariate.  

If you want to explore simulating multivariate data, [Dr. Eric Scott](https://www.ericrscott.com/) developed a R package called [Holodeck](https://github.com/Aariq/holodeck) for simulating multivariate data. 

<br>

## Sources

* Bolker, B. (2008) *Ecological models and data in R.* Princeton University Press, Princeton, New Jersey, USA.
(https://ms.mcmaster.ca/~bolker/emdbook/)   

* Burnham, K. P. and Anderson, D. R. (2002) *Model Selection and Inference: A Practical Information-Theoretic Approach.* Second Edition. Springer, New York, New York, USA.  

* Gotelli, N.J. and Ellison, A.M. (2012) *A Primer for Ecological Statistics*. Second edition, Sinauer Associates Publishers.

* Kery, M. (2010) *Introduction to WinBUGS for Ecologists.* Academic Press, Burlinton, Massachusetts, USA.  

* White G. C. , and Cooch E. (2005). *Program MARK: A Gentle Introduction.* 4th edn. (http://www.phidot.org/software/mark/docs/book/.)  

* Zuur, A.F, E.N. Ieno, and E. Meesters. (2009) *A Beginner’s Guide to R.* Springer, New York, New York, USA.   

* Zuur, A.F. (2012) *A Beginner's guide to Generalized Additive Models with R*. Highland Statistics Ltd, Newburgh, NY.